{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Inference on Google Vertex AI using TigerGraph\n",
    "\n",
    "In this notebook, we will train a GNN model and deploy it to Google Vertex AI as an inference endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We are going to create a working directory.\n",
    "**Note:** the `mkdir` command below will fail if the directory already exists. You can safely ignore the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_directory = \"gat_cora\"\n",
    "\n",
    "os.mkdir(\"./{}\".format(source_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Model\n",
    "\n",
    "We are going to define a Graph Attention Network (GAT) model, and write it to a file called `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features, num_layers, out_dim, dropout, hidden_dim, num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_units = num_features if i == 0 else hidden_dim * num_heads\n",
    "            out_units = out_dim if i == (num_layers - 1) else hidden_dim\n",
    "            heads = 1 if i == (num_layers - 1) else num_heads\n",
    "            self.layers.append(\n",
    "                GATConv(in_units, out_units, heads=heads, dropout=dropout)\n",
    "            )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.float(), data.edge_index\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Here, we define a dictionary of the parameters of the model, data loaders, and connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"model_name\": \"GAT\",\n",
    "    \"model_config\": {\n",
    "        \"num_features\": 1433, # Number of features on Cora vertices \n",
    "        \"out_dim\": 7,         # Number of classes in Cora\n",
    "        \"num_heads\": 8,       # Number of attention heads in GAT model\n",
    "        \"hidden_dim\": 8,      # Number of hidden units in GAT model\n",
    "        \"num_layers\": 2,      # Number of GAT layers in GAT model\n",
    "        \"dropout\": 0.6        # Dropout probability in GAT model\n",
    "    },\n",
    "    \"infer_loader_config\": {\n",
    "        \"v_in_feats\": [\"x\"],     # List of vertex features to be loaded\n",
    "        \"v_out_labels\": [\"y\"],   # List of vertex labels to be loaded\n",
    "        \"v_extra_feats\": [\"train_mask\",\"val_mask\",\"test_mask\"],     # Don't need any extra features for inference\n",
    "        \"output_format\": \"PyG\",  # Using Pytorch Geometric format\n",
    "        \"batch_size\": 64,        # Batch size for inference\n",
    "        \"num_neighbors\": 10,     # Number of neighbors per vertex\n",
    "        \"num_hops\": 2,           # How deep to go in the graph\n",
    "        \"shuffle\": False         # Don't shuffle the data\n",
    "    },\n",
    "    \"training_loader_config\": {\n",
    "        \"v_in_feats\": [\"x\"],\n",
    "        \"v_out_labels\": [\"y\"],\n",
    "        \"v_extra_feats\": [\"train_mask\",\"val_mask\",\"test_mask\"],\n",
    "        \"output_format\": \"PyG\",\n",
    "        \"batch_size\": 64, \n",
    "        \"num_neighbors\": 10, \n",
    "        \"num_hops\": 2,\n",
    "        \"shuffle\": True\n",
    "    },\n",
    "    \"optimizer_config\": {\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "    },\n",
    "    \"connection_config\": {\n",
    "        \"host\": \"http://3.22.188.182\", \n",
    "        \"graphname\": \"Cora\", \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Parameters to JSON File\n",
    "We will write the parameters dictionary to a JSON file so that we can easily access the parameters when creating the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(parameters, open(\"{}/config.json\".format(source_directory), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a GNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "Here, we use some Python packaging tools to load the model. This is equivalent to writing `from source_directory.model import ModelName`.\n",
    "\n",
    "Since `source_directory` and `ModelName` are unique to each developer's configs, we will use the `sys` package to import the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(source_directory)\n",
    "\n",
    "import model\n",
    "GAT = getattr(model, parameters[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the Model Class\n",
    "Here, we use `kwargs` to pass in the parameters of the model from the parameters dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GAT(**parameters[\"model_config\"])\n",
    "gat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "Here, we instantiate a connection to our TigerGraph database with `pyTigerGraph`. Then we create data loaders for training, validation, and testing datasets. We will use the **Neighbor Sampling** technique introduced in the GraphSAGE paper to generate batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ns518b6ll2c23lelf04rlgtb84vlpf15', 1659648035, '2022-08-04 21:20:35')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyTigerGraph import TigerGraphConnection\n",
    "\n",
    "conn = TigerGraphConnection(**parameters[\"connection_config\"])\n",
    "conn.getToken(conn.createSecret())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing queries. It might take a minute if this is the first time you use this loader.\n",
      "Query installation finished.\n"
     ]
    }
   ],
   "source": [
    "train_loader = conn.gds.neighborLoader(\n",
    "    **parameters[\"training_loader_config\"],\n",
    "    filter_by=\"train_mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = conn.gds.neighborLoader(\n",
    "    **parameters[\"training_loader_config\"],\n",
    "    filter_by=\"val_mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = conn.gds.neighborLoader(\n",
    "    **parameters[\"training_loader_config\"],\n",
    "    filter_by=\"test_mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Optimizer\n",
    "Here, we define the `Adam` optimizer and move the model to the correct device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gat.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    gat.parameters(), **parameters[\"optimizer_config\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyTigerGraph.gds.metrics import Accumulator, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "logs = {}\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    gat.train()\n",
    "    epoch_train_loss = Accumulator()\n",
    "    epoch_train_acc = Accuracy()\n",
    "    for bid, batch in enumerate(train_loader):\n",
    "        batchsize = batch.x.shape[0]\n",
    "        batch.to(device)\n",
    "        # Forward pass\n",
    "        out = gat(batch)\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.update(loss.item() * batchsize, batchsize)\n",
    "        # Predict on training data\n",
    "        with torch.no_grad():\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_train_acc.update(pred[batch.train_mask], batch.y[batch.train_mask])\n",
    "        # Log training status after each batch\n",
    "        logs[\"loss\"] = epoch_train_loss.mean\n",
    "        logs[\"acc\"] = epoch_train_acc.value\n",
    "        print(\n",
    "            \"Epoch {}, Train Batch {}, Loss {:.4f}, Accuracy {:.4f}\".format(\n",
    "                epoch, bid, logs[\"loss\"], logs[\"acc\"]\n",
    "            )\n",
    "        )\n",
    "        global_steps += 1\n",
    "    # Evaluate\n",
    "    gat.eval()\n",
    "    epoch_val_loss = Accumulator()\n",
    "    epoch_val_acc = Accuracy()\n",
    "    for batch in valid_loader:\n",
    "        batchsize = batch.x.shape[0]\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            out = gat(batch)\n",
    "            # Calculate loss\n",
    "            valid_loss = F.cross_entropy(out[batch.val_mask], batch.y[batch.val_mask])\n",
    "            epoch_val_loss.update(valid_loss.item() * batchsize, batchsize)\n",
    "            # Prediction\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_val_acc.update(pred[batch.val_mask], batch.y[batch.val_mask])\n",
    "    # Log testing result after each epoch\n",
    "    logs[\"val_loss\"] = epoch_val_loss.mean\n",
    "    logs[\"val_acc\"] = epoch_val_acc.value\n",
    "    print(\n",
    "        \"Epoch {}, Valid Loss {:.4f}, Valid Accuracy {:.4f}\".format(\n",
    "            epoch, logs[\"val_loss\"], logs[\"val_acc\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat.eval()\n",
    "acc = Accuracy()\n",
    "for batch in test_loader:\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = gat(batch).argmax(dim=1)\n",
    "        acc.update(pred[batch.test_mask], batch.y[batch.test_mask])\n",
    "print(\"Accuracy: {:.4f}\".format(acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Trained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gat.state_dict(), \"{}/model.pth\".format(source_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dockerfile\n",
    "\n",
    "Google Vertex AI uses Docker containers in order to host models. We use a Dockerfile to build this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM ubuntu:latest\n",
    "\n",
    "# Install some basic utilities\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    curl \\\n",
    "    ca-certificates \\\n",
    "    sudo \\\n",
    "    git \\\n",
    "    bzip2 \\\n",
    "    libx11-6 \\\n",
    "    wget \\\n",
    "    pip \\\n",
    " && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "WORKDIR /opt\n",
    "# Set up the Conda environment\n",
    "ENV CONDA_AUTO_UPDATE_CONDA=false \\\n",
    "    PATH=/opt/miniconda/bin:$PATH\n",
    "COPY ./gat_cora/environment.yml /opt/environment.yml\n",
    "RUN curl -sLo /opt/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-py39_4.11.0-Linux-x86_64.sh \\\n",
    " && chmod +x /opt/miniconda.sh \\\n",
    " && /opt/miniconda.sh -b -p /opt/miniconda \\\n",
    " && rm /opt/miniconda.sh \\\n",
    " && conda env update -n base -f /opt/environment.yml \\\n",
    " && rm /opt/environment.yml \\\n",
    " && conda clean -ya\n",
    "\n",
    " RUN pip install --no-index torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html \\\n",
    " && pip install --no-index torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html \\\n",
    " && pip install --no-index torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+cu113.html \\\n",
    " && pip install --no-index torch-spline-conv -f https://data.pyg.org/whl/torch-1.10.0+cu113.html \\\n",
    " && pip install torch-geometric \\\n",
    " && pip cache purge\n",
    "\n",
    "# install - requirements.txt\n",
    "COPY ./gat_cora/requirements.txt /tmp/requirements.txt\n",
    "RUN python3 -m pip install -r /tmp/requirements.txt --quiet --no-cache-dir \\\n",
    "  && rm -f /tmp/requirements.txt\n",
    "\n",
    "ENV TARGET_DIR /opt/kserve-demo\n",
    "WORKDIR ${TARGET_DIR}\n",
    "COPY ./gat_cora/ ${TARGET_DIR}/gat_cora/\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"./gat_cora/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define main.py File\n",
    "\n",
    "This `main.py` file will load the model and start running an HTTP server for model inference within the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/main.py\n",
    "import torch\n",
    "import kserve\n",
    "from google.cloud import storage\n",
    "# from sklearn.externals import joblib\n",
    "from kserve import Model, Storage\n",
    "from kserve.model import ModelMissingError, InferenceError\n",
    "from typing import Dict\n",
    "import logging\n",
    "import pyTigerGraph as tg\n",
    "import os \n",
    "import sys\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VertexClassifier(Model):\n",
    "    def __init__(self, name: str, source_directory: str):\n",
    "        super().__init__(name)\n",
    "        self.name = name\n",
    "        self.source_dir = source_directory\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "        # Load configuration JSON file\n",
    "        with open(os.path.join(source_directory, \"config.json\")) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            self.model_config = data[\"model_config\"]\n",
    "            connection_config = data[\"connection_config\"]\n",
    "            loader_config = data[\"infer_loader_config\"]\n",
    "            self.mdl_nm = data[\"model_name\"]\n",
    "\n",
    "        sys.path.append(source_directory)\n",
    "        # Setup Connection to TigerGraph Database\n",
    "        self.conn = tg.TigerGraphConnection(**connection_config)\n",
    "\n",
    "        # Setup Inference Loader\n",
    "        self.infer_loader = self.conn.gds.neighborLoader(**loader_config)\n",
    "\n",
    "        # Setup Model\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "    \n",
    "    def load_model(self):\n",
    "        import model\n",
    "        mdl = getattr(model, self.mdl_nm)(**self.model_config)\n",
    "        logger.info(\"Instantiated Model\")\n",
    "        with open(os.path.join(self.source_dir, \"model.pth\"), 'rb') as f:\n",
    "            mdl.load_state_dict(torch.load(f))\n",
    "        mdl.to(self.device).eval()\n",
    "        logger.info(\"Loaded Model\")\n",
    "        return mdl\n",
    "\n",
    "    def predict(self, request: Dict) -> Dict:\n",
    "        input_nodes = request[\"instances\"]\n",
    "        input_ids = set([str(node['primary_id']) for node in input_nodes])\n",
    "        logger.info(input_ids)\n",
    "        data = self.infer_loader.fetch(input_nodes).to(self.device)\n",
    "        logger.info (f\"predicting {data}\")\n",
    "        with torch.no_grad():\n",
    "            output = self.model(data)\n",
    "        returnJSON = []\n",
    "        for i in range(len(input_nodes)):\n",
    "            returnJSON.append({input_nodes[i][\"primary_id\"]: list(output[i].tolist())})\n",
    "        return json.dumps({\"predictions\": returnJSON})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = os.environ.get('K_SERVICE', \"tg-gat-gcp-demo-predictor-default\")\n",
    "    model_name = '-'.join(model_name.split('-')[:-2]) # removing suffix \"-predictor-default\"\n",
    "    print(model_name)\n",
    "    logging.info(f\"Starting model '{model_name}'\")\n",
    "    model = VertexClassifier(model_name, \"./gat_cora/\")\n",
    "    kserve.ModelServer(http_port=8080).start([model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write requirements.txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/requirements.txt\n",
    "\n",
    "# kubeflow packages\n",
    "kfp==1.6.3\n",
    "kfp-server-api==1.6.0\n",
    "kserve==0.8\n",
    "\n",
    "# common packages\n",
    "#bokeh==2.3.2\n",
    "#cloudpickle==1.6.0\n",
    "#dill==0.3.4\n",
    "#pandas==1.2.4\n",
    "\n",
    "# pytorch packages\n",
    "#fastai==2.4\n",
    "class-resolver==0.3.9\n",
    "\n",
    "# TigerGraph\n",
    "pyTigerGraph[gds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write environment.yml File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/environment.yml\n",
    "name: base\n",
    "dependencies:\n",
    "- numpy=1.21.2\n",
    "- pip=21.2.4\n",
    "- python=3.9.7\n",
    "- pytorch::pytorch=1.10.0=py3.9_cuda11.3_cudnn8.2.0_0\n",
    "- scipy=1.7.1\n",
    "- cloudpickle=2.0.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker Image\n",
    "Using the Dockerfile defined above, we will build the Docker image for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --rm --platform linux/amd64 -t kserve-base:1.0 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Docker Image Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 8080:8080 kserve-base:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Local Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = {\"instances\": [{\"primary_id\": 7, \"type\": \"Paper\"}, {\"primary_id\": 17, \"type\": \"Paper\"}, {\"primary_id\": 27, \"type\": \"Paper\"}, {\"primary_id\": 37, \"type\": \"Paper\"}]}\n",
    "\n",
    "resp = requests.post(\"http://localhost:8080/v1/models/tg-gat-gcp-demo:predict\", json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:8080/v1/models\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag kserve-base:1.0 us-central1-docker.pkg.dev/tigergraph-ml/gnn-inference/cora-gat-inference:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker push us-central1-docker.pkg.dev/tigergraph-ml/gnn-inference/cora-gat-inference:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai models upload \\\n",
    "  --region=us-central1 \\\n",
    "  --display-name=cora-gat \\\n",
    "  --container-image-uri=us-central1-docker.pkg.dev/tigergraph-ml/gnn-inference/cora-gat-inference:latest \\\n",
    "  --container-health-route=/v1/models \\\n",
    "  --container-predict-route=/v1/models/tg-gat-gcp-demo:predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai models list --region=us-central1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai endpoints create --region=us-central1 --display-name=coragat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai endpoints list --region=us-central1 --filter=display_name=coragat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai endpoints deploy-model YOUR_ENDPOINT_ID\\\n",
    "  --region=us-central1 \\\n",
    "  --model=YOUR_MODEL_ID \\\n",
    "  --display-name=coragat \\\n",
    "  --machine-type=n1-standard-2 \\\n",
    "  --min-replica-count=1 \\\n",
    "  --max-replica-count=5 \\\n",
    "  --traffic-split=0=100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(data, open(\"request.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ENDPOINT_ID=\"YOUR_ENDPOINT_ID\"\n",
    "!PROJECT_ID=\"tigergraph-ml\"\n",
    "!INPUT_DATA_FILE=\"request.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \\                           \n",
    "-X POST \\                 \n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict \\\n",
    "-d \"@${INPUT_DATA_FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9303e460a50dcee59d9445c0aaef0cdf9fe40f239a9daa8cbf9d6558359a1370"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('azure-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

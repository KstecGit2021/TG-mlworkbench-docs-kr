{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Inference on Azure ML using TigerGraph\n",
    "\n",
    "In this notebook, we will train a GNN model and deploy it to Azure ML as an inference endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We are going to create a working directory that will eventually be uploaded to the Azure inference endpoint. **Note:** the `mkdir` command below will fail if the directory already exists. You can safely ignore the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_directory = \"gat_cora\"\n",
    "\n",
    "os.mkdir(\"./{}\".format(source_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Model\n",
    "\n",
    "We are going to define a Graph Attention Network (GAT) model, and write it to a file called `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features, num_layers, out_dim, dropout, hidden_dim, num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_units = num_features if i == 0 else hidden_dim * num_heads\n",
    "            out_units = out_dim if i == (num_layers - 1) else hidden_dim\n",
    "            heads = 1 if i == (num_layers - 1) else num_heads\n",
    "            self.layers.append(\n",
    "                GATConv(in_units, out_units, heads=heads, dropout=dropout)\n",
    "            )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.float(), data.edge_index\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Here, we define a dictionary of the parameters of the model, data loaders, and connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"model_name\": \"GAT\",\n",
    "    \"model_config\": {\n",
    "        \"num_features\": 1433, # Number of features on Cora vertices \n",
    "        \"out_dim\": 7,         # Number of classes in Cora\n",
    "        \"num_heads\": 8,       # Number of attention heads in GAT model\n",
    "        \"hidden_dim\": 8,      # Number of hidden units in GAT model\n",
    "        \"num_layers\": 2,      # Number of GAT layers in GAT model\n",
    "        \"dropout\": 0.6        # Dropout probability in GAT model\n",
    "    },\n",
    "    \"infer_loader_config\": {\n",
    "        \"v_in_feats\": [\"x\"],     # List of vertex features to be loaded\n",
    "        \"v_out_labels\": [\"y\"],   # List of vertex labels to be loaded\n",
    "        \"v_extra_feats\": [],     # Don't need any extra features for inference\n",
    "        \"output_format\": \"PyG\",  # Using Pytorch Geometric format\n",
    "        \"batch_size\": 64,        # Batch size for inference\n",
    "        \"num_neighbors\": 10,     # Number of neighbors per vertex\n",
    "        \"num_hops\": 2,           # How deep to go in the graph\n",
    "        \"shuffle\": False         # Don't shuffle the data\n",
    "    },\n",
    "    \"training_loader_config\": {\n",
    "        \"v_in_feats\": [\"x\"],\n",
    "        \"v_out_labels\": [\"y\"],\n",
    "        \"v_extra_feats\": [\"train_mask\",\"val_mask\",\"test_mask\"],\n",
    "        \"output_format\": \"PyG\",\n",
    "        \"batch_size\": 64, \n",
    "        \"num_neighbors\": 10, \n",
    "        \"num_hops\": 2,\n",
    "        \"shuffle\": True\n",
    "    },\n",
    "    \"optimizer_config\": {\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "    },\n",
    "    \"connection_config\": {\n",
    "        \"host\": \"http://35.230.92.92\", \n",
    "        \"graphname\": \"Cora\", \n",
    "        \"username\": \"tigergraph\", \n",
    "        \"password\": \"tigergraph\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Parameters to JSON File\n",
    "We will write the parameters dictionary to a JSON file so that we can easily access the parameters when creating the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(parameters, open(\"{}/config.json\".format(source_directory), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a GNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "Here, we use some Python packaging tools to load the model. This is equivalent to writing `from source_directory.model import ModelName`.\n",
    "\n",
    "Since `source_directory` and `ModelName` are unique to each developer's configs, we will use the `sys` package to import the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(source_directory)\n",
    "\n",
    "import model\n",
    "GAT = getattr(model, parameters[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the Model Class\n",
    "Here, we use `kwargs` to pass in the parameters of the model from the parameters dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GAT(**parameters[\"model_config\"])\n",
    "gat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "Here, we instantiate a connection to our TigerGraph database with `pyTigerGraph`. Then we create data loaders for training, validation, and testing datasets. We will use the **Neighbor Sampling** technique introduced in the GraphSAGE paper to generate batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyTigerGraph import TigerGraphConnection\n",
    "\n",
    "conn = TigerGraphConnection(**parameters[\"connection_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = conn.gds.neighborLoader(\n",
    "    **parameters[\"training_loader_config\"],\n",
    "    filter_by=\"train_mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = conn.gds.neighborLoader(\n",
    "    **parameters[\"training_loader_config\"],\n",
    "    filter_by=\"val_mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = conn.gds.neighborLoader(\n",
    "    **parameters[\"training_loader_config\"],\n",
    "    filter_by=\"test_mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Optimizer\n",
    "Here, we define the `Adam` optimizer and move the model to the correct device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gat.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    gat.parameters(), **parameters[\"optimizer_config\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyTigerGraph.gds.metrics import Accumulator, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "logs = {}\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    gat.train()\n",
    "    epoch_train_loss = Accumulator()\n",
    "    epoch_train_acc = Accuracy()\n",
    "    for bid, batch in enumerate(train_loader):\n",
    "        batchsize = batch.x.shape[0]\n",
    "        batch.to(device)\n",
    "        # Forward pass\n",
    "        out = gat(batch)\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.update(loss.item() * batchsize, batchsize)\n",
    "        # Predict on training data\n",
    "        with torch.no_grad():\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_train_acc.update(pred[batch.train_mask], batch.y[batch.train_mask])\n",
    "        # Log training status after each batch\n",
    "        logs[\"loss\"] = epoch_train_loss.mean\n",
    "        logs[\"acc\"] = epoch_train_acc.value\n",
    "        print(\n",
    "            \"Epoch {}, Train Batch {}, Loss {:.4f}, Accuracy {:.4f}\".format(\n",
    "                epoch, bid, logs[\"loss\"], logs[\"acc\"]\n",
    "            )\n",
    "        )\n",
    "        global_steps += 1\n",
    "    # Evaluate\n",
    "    gat.eval()\n",
    "    epoch_val_loss = Accumulator()\n",
    "    epoch_val_acc = Accuracy()\n",
    "    for batch in valid_loader:\n",
    "        batchsize = batch.x.shape[0]\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            out = gat(batch)\n",
    "            # Calculate loss\n",
    "            valid_loss = F.cross_entropy(out[batch.val_mask], batch.y[batch.val_mask])\n",
    "            epoch_val_loss.update(valid_loss.item() * batchsize, batchsize)\n",
    "            # Prediction\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_val_acc.update(pred[batch.val_mask], batch.y[batch.val_mask])\n",
    "    # Log testing result after each epoch\n",
    "    logs[\"val_loss\"] = epoch_val_loss.mean\n",
    "    logs[\"val_acc\"] = epoch_val_acc.value\n",
    "    print(\n",
    "        \"Epoch {}, Valid Loss {:.4f}, Valid Accuracy {:.4f}\".format(\n",
    "            epoch, logs[\"val_loss\"], logs[\"val_acc\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat.eval()\n",
    "acc = Accuracy()\n",
    "for batch in test_loader:\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = gat(batch).argmax(dim=1)\n",
    "        acc.update(pred[batch.test_mask], batch.y[batch.test_mask])\n",
    "print(\"Accuracy: {:.4f}\".format(acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Trained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gat.state_dict(), \"{}/model.pth\".format(source_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Inference Container for Azure ML\n",
    "Using the Azure ML tools, we will define the inference container. The Azure ML libraries will take care of building the container, but we need to define the custom inference script and environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Scoring File\n",
    "Azure ML requires a file called `score.py` to run inference. This file will load the model, model configuration, and model weights, and process requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/score.py\n",
    "\n",
    "import pyTigerGraph as tg\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def init():\n",
    "    # Configure device usage\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load configuration JSON file\n",
    "    with open(os.path.join(os.getenv(\"AZUREML_SOURCE_DIR\"), \"config.json\")) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        model_config = data[\"model_config\"]\n",
    "        connection_config = data[\"connection_config\"]\n",
    "        loader_config = data[\"infer_loader_config\"]\n",
    "        model_name = data[\"model_name\"]\n",
    "\n",
    "    sys.path.append(os.getenv(\"AZUREML_SOURCE_DIR\"))\n",
    "\n",
    "    # Load model definition\n",
    "    import model\n",
    "    global mdl\n",
    "    mdl = getattr(model, model_name)(**model_config)\n",
    "\n",
    "    # Setup Connection to TigerGraph Database\n",
    "    global conn\n",
    "    conn = tg.TigerGraphConnection(**connection_config)\n",
    "\n",
    "    # Load the trained model weights\n",
    "    with open(os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"model.pth\"), 'rb') as f:\n",
    "        mdl.load_state_dict(torch.load(f))\n",
    "    mdl.to(device).eval()\n",
    "\n",
    "    # Configure data loader\n",
    "    global infer_loader\n",
    "    infer_loader = conn.gds.neighborLoader(**loader_config)\n",
    "\n",
    "\n",
    "def run(request):\n",
    "    # load data from JSON request\n",
    "    input_data = json.loads(request)\n",
    "    \n",
    "    # fetch subgraphs for JSON requests\n",
    "    sub_graphs = infer_loader.fetch(input_data[\"vertices\"])\n",
    "\n",
    "    # move data to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mdl.to(device)\n",
    "    sub_graphs.to(device)\n",
    "\n",
    "    # run inference\n",
    "    with torch.no_grad():\n",
    "        output = mdl(sub_graphs)\n",
    "\n",
    "    # process and return results\n",
    "    returnJSON = {}\n",
    "    for i in range(len(input_data[\"vertices\"])):\n",
    "        returnJSON[input_data[\"vertices\"][i][\"primary_id\"]] = list(output[i].tolist())\n",
    "    return returnJSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Workspace\n",
    "First, we are going to create an Azure ML workspace. This requires an Azure subscription id and a resource group name. You can define these in a `config.py` file that follows the same format as `config-temp.py` in this repository. The `location` parameter in the `Workspace.create()` function will need to be changed to your own Azure instance location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import config as cfg\n",
    "\n",
    "ws = Workspace.create(name=\"gat_cora\",\n",
    "                      subscription_id=cfg.subscription_id,\n",
    "                      resource_group=cfg.resource_group,\n",
    "                      create_resource_group=True,\n",
    "                      location=\"eastus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Model\n",
    "Here, we register the model with Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "model = Model.register(ws,\n",
    "                       model_name=\"cora-gat\",\n",
    "                       model_path=\"{}/model.pth\".format(source_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "Here, we define the environment variables and the Python packages that will be used for the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(name='cora-gat')\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"pyTigerGraph\")\n",
    "\n",
    "conda_dep.add_channel(\"pytorch\")\n",
    "conda_dep.add_conda_package(\"cpuonly\")\n",
    "\n",
    "conda_dep.add_channel(\"pyg\")\n",
    "conda_dep.add_conda_package(\"pyg\")\n",
    "\n",
    "env.environment_variables = {'AZUREML_SOURCE_DIR': source_directory}\n",
    "env.python.conda_dependencies = conda_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inference Config\n",
    "Here, we define the configuration of the inference container. We will pass in the environment, source directory, and the scoring file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(environment=env, \n",
    "                                   source_directory=source_directory,\n",
    "                                   entry_script='./score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model Locally\n",
    "\n",
    "Using the Azure ML tools, we will first deploy the trained model locally. This will allow us to test the model before deploying it to Azure. This may take a while due to needing to download and build a Docker image. **Prequisite:** You must have Docker installed on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "\n",
    "\n",
    "service = Model.deploy(\n",
    "    ws,\n",
    "    \"cora-gat\",\n",
    "    [model],\n",
    "    inference_config,\n",
    "    deployment_config,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print(service.get_logs())\n",
    "print(\"URL: {}\".format(service.scoring_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Local Model\n",
    "Here, we call the local REST endpoint to make predictions. Note that we return the raw logits output from the GAT model, which are not normalized. We could run the logits through a Softmax function to get the probabilities, but the index with the greatest number is the most likely class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "uri = \"http://localhost:6789/score\" # URL of local web service\n",
    "requests.get(\"http://localhost:6789\")\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# The scoring function assumes a JSON file with a list of vertices with their types.\n",
    "data = {\"vertices\": [{\"primary_id\": \"100\", \"type\": \"Paper\"},\n",
    "                    {\"primary_id\": \"55\", \"type\": \"Paper\"}]}\n",
    "\n",
    "data = json.dumps(data)\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Azure ML\n",
    "Once we verify the model locally, we can deploy it to Azure ML.\n",
    "\n",
    "Azure ML can provide authentication on the REST endpoint. If desired, you can change the `auth` boolean variable below to `True` to enable authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = False\n",
    "\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=2, memory_gb=8, auth_enabled=auth\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    ws,\n",
    "    \"cora-gat\",\n",
    "    [model],\n",
    "    inference_config,\n",
    "    deployment_config,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Model on Azure ML\n",
    "Once deployed, we can call the endpoint hosted on Azure ML. Once again, note that we return the raw logits output from the GAT model, which are not normalized. We could run the logits through a Softmax function to get the probabilities, but the index with the greatest number is the most likely class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Webservice\n",
    "\n",
    "service = Webservice(workspace=ws, name=\"cora-gat\")\n",
    "scoring_uri = service.scoring_uri\n",
    "    \n",
    "# Set the appropriate headers\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "if auth:\n",
    "    key, _ = service.get_keys()\n",
    "    headers[\"Authorization\"] = f\"Bearer {key}\"\n",
    "\n",
    "# Make the request and display the response and logs\n",
    "data = {\"vertices\": [{\"primary_id\": \"100\", \"type\": \"Paper\"},\n",
    "                    {\"primary_id\": \"55\", \"type\": \"Paper\"}]}\n",
    "\n",
    "data = json.dumps(data)\n",
    "resp = requests.post(scoring_uri, data=data, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9303e460a50dcee59d9445c0aaef0cdf9fe40f239a9daa8cbf9d6558359a1370"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('azure-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
